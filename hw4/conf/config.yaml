
env_name: 'LunarLander-v3' # ['LunarLander-v3', 'MsPacman-v0', 'HalfCheetah-v2', 'CartPole-v0']
atari: False
ep_len: 200
exp_name: 'todo'
double_q: True
batch_size: 64 ## The min amount of experience to collect before a training update
train_batch_size: 64 ## training batch size used for computing gradients of q function or policy
eval_batch_size: 4096 ## How much experience should be collected over the environment to evaluate the average reward of a policy
num_agent_train_steps_per_iter: 2 ## Number of training updates after #batch_size experience is collected. 
num_critic_updates_per_agent_update: 2 ## Number of training updates after #batch_size experience is collected.
seed: 1
no_gpu: False
which_gpu: 0
video_log_freq: -1
scalar_log_freq: -1
save_params: True
rl_alg: 'dqn' ## RL training algorithm ['dqn', 'ddpg', 'td3']
learning_starts: 1024  ## How much initial experience to collect before training begins
learning_freq: 0 
target_update_freq: 1
exploration_schedule: 0
optimizer_spec:  0
replay_buffer_size: 1000000
frame_history_len: 1
gamma: 0.99
n_layers_critic: 2
size_hidden_critic: 64
critic_learning_rate: 1e-3
n_layers: 2
size: 64
learning_rate: 1e-3
ob_dim: 0
ac_dim: 0
discrete: True
grad_norm_clipping: True
n_iter: 1000
polyak_avg: 0.01 ##
td3_target_policy_noise: 0.1 ## 

   




























  
  

